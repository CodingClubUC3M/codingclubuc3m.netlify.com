---
title: "Stan: Probabilistic Modeling and Bayesian Inference"
authors: ["J. Miguel Marín"]
date: 2019-01-22
categories: ["R"]
tags: ["Stan", "Bayesian", "Functional programming"]
---



<p>Stan is a probabilistic programming language for specifying statistical models. Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan can be called through R using the rstan package, and through Python using the pystan package. Both interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. In this talk it is shown a brief glance about the main properties of Stan. It is shown, also a couple of examples: first one related with a simple Bernoulli model and second one, about a Lotka-Volterra model based on ordinary differential equations.</p>
<hr />
<div id="development-team-of-stan" class="section level4">
<h4>Development Team of STAN</h4>
<p><strong>Andrew Gelman</strong>
Bob Carpenter, Daniel Lee, Ben Goodrich,
Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Allen Riddell,
Marco Inacio, Jeffrey Arnold, Mitzi Morris, Rob Trangucci,
Rob Goedman, Brian Lau, Jonah Sol Gabry, Robert L. Grant,
Krzysztof Sakrejda, Aki Vehtari, Rayleigh Lei, Sebastian Weber,
Charles Margossian, Vincent Picaud, Imad Ali, Sean Talts,
Ben Bales, Ari Hartikainen, Matthijs Vakar, Andrew Johnson,
Dan Simpson</p>
<hr />
</div>
<div id="what-is-stan" class="section level4">
<h4>What is Stan?</h4>
<ul>
<li>Stan is named in honor of <em>Stanislaw Ulam</em> (1909-1984): Co-inventor of the Monte Carlo method</li>
<li>Stan is an imperative probabilistic programming language</li>
<li>Stan program: defines a probability model
<ul>
<li>declares data and (constrained) parameter variables</li>
<li>defines log posterior (or penalized likelihood)</li>
</ul></li>
<li>Stan inference: fits model to data &amp; makes predictions
<ul>
<li>MCMC for full Bayesian inference</li>
<li>VB for approximate Bayesian inference</li>
<li>MLE for penalized maximum likelihood estimation</li>
</ul></li>
</ul>
<hr />
</div>
<div id="what-stan-computes" class="section level4">
<h4>WHAT STAN COMPUTES</h4>
<ul>
<li>Draws from <strong>posterior distributions</strong></li>
<li>Stan performs Markov chain Monte Carlo sampling</li>
<li>Produces sequence of draws <span class="math display">\[\theta_{(1)} ,\theta_{(2)}, \ldots, \theta_{(M)}\]</span></li>
<li>where each draw <span class="math inline">\(\theta_{(i)}\)</span> is marginally distributed according to the posterior <span class="math inline">\(p(\theta|y)\)</span></li>
<li>Draws characterize posterior distributions</li>
<li>Plot with histograms, kernel density estimates, etc.</li>
<li>See a quick look about rstan
<a href="https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started">Getting Started</a></li>
</ul>
<hr />
</div>
<div id="install-rstan" class="section level4">
<h4>INSTALL <code>rstan</code></h4>
<ul>
<li>Before installing <code>rstan</code> it is necessary to install Rtools</li>
<li><p>See
<a href="https://cran.r-project.org/bin/windows/Rtools">Rtools stuff</a></p></li>
<li>Check whether the path (in Windows) is correctly fixed for all binaries of Rtools.</li>
<li><p>If it is not the case, write in R:</p></li>
</ul>
<pre><code>install.packages(devtools)
library(devtools)
Sys.setenv(PATH = paste(&quot;C:\\Rtools\\bin&quot;, Sys.getenv(&quot;PATH&quot;), sep=&quot;;&quot;))
Sys.setenv(PATH = paste(&quot;C:\\Rtools\\mingw_64\\bin&quot;, Sys.getenv(&quot;PATH&quot;), sep=&quot;;&quot;))</code></pre>
<ul>
<li>Then install <code>rstan</code></li>
</ul>
<pre><code>install.packages(rstan)</code></pre>
<hr />
</div>
<div id="basic-syntax-in-stan" class="section level4">
<h4>BASIC SYNTAX in STAN</h4>
<p>A Stan model is defined by five <strong>program blocks data</strong></p>
<ul>
<li>transformed data</li>
<li>parameters (required)</li>
<li>transformed parameters</li>
<li>model (required)</li>
<li>generated quantities</li>
</ul>
<hr />
<ul>
<li>The <strong>data</strong> block reads external information</li>
</ul>
<pre><code>data {
  int N;
  int x[N];
  int offset;
}</code></pre>
<hr />
<ul>
<li>The <strong>transformed data</strong> block allows for preprocessing of the data</li>
</ul>
<pre><code>transformed data {
  int y[N];
  for (n in 1:N)
    y[n] = x[n] - offset;
}</code></pre>
<hr />
<ul>
<li>The <strong>parameters</strong> block defines the sampling space</li>
</ul>
<pre><code>parameters {
real&lt;lower=0&gt; lambda1;
real&lt;lower=0&gt; lambda2;
}</code></pre>
<hr />
<ul>
<li>The <strong>transformed parameters</strong> block allows for parameter processing before the posterior is computed</li>
</ul>
<pre><code>transformed parameters {
real&lt;lower=0&gt; lambda;
lambda = lambda1 + lambda2;
}</code></pre>
<hr />
<ul>
<li>In the <strong>model</strong> block we define our posterior distributions</li>
</ul>
<pre><code>model {
y ~ poisson(lambda);
lambda1 ~ cauchy(0, 2.5);
lambda2 ~ cauchy(0, 2.5);
}</code></pre>
<hr />
<ul>
<li>Lastly, the <strong>generated quantities</strong> block allows for postprocessing</li>
</ul>
<pre><code>generated quantities {
int x_predict;
x_predict = poisson_rng(lambda) + offset;
}</code></pre>
<hr />
<ul>
<li>Stan has two primitive <strong>types</strong> and both can be bounded.
<ul>
<li><strong>int</strong> is an integer type</li>
<li><strong>real</strong> is a floating point precision type</li>
</ul></li>
</ul>
<pre><code>int&lt;lower=1&gt; N;

real&lt;upper=5&gt; alpha;
real&lt;lower=-1,upper=1&gt; beta;

real gamma;
real&lt;upper=gamma&gt; zeta;</code></pre>
<hr />
<ul>
<li>Reals extend to linear algebra types</li>
</ul>
<pre><code>vector[10] a;     // Column vector
matrix[10, 1] b;

row_vector[10] c; // Row vector
matrix[1, 10] d;</code></pre>
<ul>
<li>Arrays of int, reals, vectors, and matrices are available</li>
</ul>
<pre><code>real a[10];

vector[10] b;

matrix[10, 10] c;</code></pre>
<ul>
<li>Stan also implements a variety of constrained types</li>
</ul>
<pre><code>simplex[5] theta;        // sum(theta) = 1

ordered[5] o;            // o[1] &lt; ... &lt; o[5]
positive_ordered[5] p;

corr_matrix[5] C;        // Symmetric and
cov_matrix[5] Sigma;     // positive-definite</code></pre>
<hr />
<ul>
<li>All the typical control and loop statements are available, too</li>
</ul>
<pre><code>if/then/else

for (i in 1:I)

while (i &lt; I)</code></pre>
<ul>
<li>There are two ways to modify the posterior</li>
</ul>
<pre><code>y ~ normal(0, 1);

increment_log_posterior(log_normal(y, 0, 1));</code></pre>
<hr />
<ul>
<li>Many sampling statements are <em>vectorized</em></li>
</ul>
<pre><code>parameters {
real mu[N];
real&lt;lower=0&gt; sigma[N];
}


model {
 // for (n in 1:N)
 // y[n] ~ normal(mu[n], sigma[n]); 

y ~ normal(mu, sigma);  // Vectorized version
}</code></pre>
<hr />
</div>
<div id="bayesian-approach" class="section level4">
<h4>BAYESIAN APPROACH</h4>
<p>Probability is <strong>Epistemic</strong></p>
<ul>
<li><em>John Stuart Mill</em> (Logic 1882, Part III, Ch. 2):
<ul>
<li>… the probability of an event is not a quality of the event itself, but a mere name for the degree of ground which we, or some one else, have for expecting it.</li>
</ul></li>
<li>Every event is in itself certain, not probable; if we knew all, we should either know positively that it will happen, or positively that it will not.</li>
<li>… its probability to us means the degree of expectation of its occurrence, which we are warranted in entertaining by our present evidence.
<ul>
<li>Probabilities quantify uncertainty</li>
<li>Statistical reasoning is <em>counterfactual</em></li>
</ul></li>
</ul>
<hr />
</div>
<div id="repeated-binary-trial-model" class="section level4">
<h4>Repeated Binary Trial Model</h4>
<ul>
<li><strong>Data</strong>
<ul>
<li><span class="math inline">\(N\in \{0,1,\ldots \}\)</span></li>
<li><span class="math inline">\(y_{n}\in \{0,1\}\)</span> trial <em>n</em> success (known, modeled data)</li>
</ul></li>
<li><strong>Parameter</strong>
<ul>
<li><span class="math inline">\(\theta \in \lbrack 0,1]\)</span>: chance of success (unknown)</li>
</ul></li>
<li><strong>Prior</strong>
<ul>
<li><span class="math inline">\(p(\theta) = Uniform(\theta|0,1) = 1\)</span></li>
</ul></li>
</ul>
<hr />
</div>
<div id="repeated-binary-trial-model-1" class="section level4">
<h4>Repeated Binary Trial Model</h4>
<ul>
<li><strong>Likelihood</strong></li>
</ul>
<p><span class="math display">\[p(y|\theta )=\prod_{n=1}^{N}Bernoulli(y_{n}|\theta
)=Nn=\prod_{n=1}^{N}\theta ^{y_{n}}(1-\theta )^{1-y_{n}}\]</span></p>
<hr />
<ul>
<li><strong>Posterior</strong></li>
</ul>
<p><span class="math display">\[p(\theta |y)\propto p(\theta )p(y|\theta )\]</span></p>
<hr />
</div>
<div id="stan-program" class="section level4">
<h4>Stan Program</h4>
<pre class="r"><code>bern.stan = 
&quot;
data {
int&lt;lower=0&gt; N;                // number of trials
int&lt;lower=0, upper=1&gt; y[N];    // success on trial n
}

parameters {
real&lt;lower=0, upper=1&gt; theta;  // chance of success
}

model {
  theta ~ uniform(0, 1);       // prior
  y ~ bernoulli(theta);        // likelihood
}
&quot;</code></pre>
<hr />
</div>
<div id="r-simulate-some-data" class="section level4">
<h4>R: Simulate some data</h4>
<pre class="r"><code># Generate data
 theta = 0.30
 N = 20
 y = rbinom(N, 1, 0.3)
 y</code></pre>
<pre><code>##  [1] 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1</code></pre>
<hr />
<ul>
<li>Calculate MLE as sample mean from data</li>
</ul>
<pre class="r"><code> sum(y) / N</code></pre>
<pre><code>## [1] 0.4</code></pre>
<hr />
</div>
<div id="rstan-bayesian-posterior" class="section level4">
<h4>RStan: Bayesian Posterior</h4>
<pre class="r"><code>library(rstan)</code></pre>
<pre><code>## Loading required package: ggplot2</code></pre>
<pre><code>## Loading required package: StanHeaders</code></pre>
<pre><code>## rstan (Version 2.18.2, GitRev: 2e1f913d3ca3)</code></pre>
<pre><code>## For execution on a local, multicore CPU with excess RAM we recommend calling
## options(mc.cores = parallel::detectCores()).
## To avoid recompilation of unchanged Stan programs, we recommend calling
## rstan_options(auto_write = TRUE)</code></pre>
<pre class="r"><code>fit = stan(model_code=bern.stan, data = list(y = y, N = N), iter=5000)</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;bfd58553d9dcc0c7cf662698c8aa5414&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 5e-06 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 5000 [  0%]  (Warmup)
## Chain 1: Iteration:  500 / 5000 [ 10%]  (Warmup)
## Chain 1: Iteration: 1000 / 5000 [ 20%]  (Warmup)
## Chain 1: Iteration: 1500 / 5000 [ 30%]  (Warmup)
## Chain 1: Iteration: 2000 / 5000 [ 40%]  (Warmup)
## Chain 1: Iteration: 2500 / 5000 [ 50%]  (Warmup)
## Chain 1: Iteration: 2501 / 5000 [ 50%]  (Sampling)
## Chain 1: Iteration: 3000 / 5000 [ 60%]  (Sampling)
## Chain 1: Iteration: 3500 / 5000 [ 70%]  (Sampling)
## Chain 1: Iteration: 4000 / 5000 [ 80%]  (Sampling)
## Chain 1: Iteration: 4500 / 5000 [ 90%]  (Sampling)
## Chain 1: Iteration: 5000 / 5000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.012953 seconds (Warm-up)
## Chain 1:                0.013621 seconds (Sampling)
## Chain 1:                0.026574 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;bfd58553d9dcc0c7cf662698c8aa5414&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 3e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 5000 [  0%]  (Warmup)
## Chain 2: Iteration:  500 / 5000 [ 10%]  (Warmup)
## Chain 2: Iteration: 1000 / 5000 [ 20%]  (Warmup)
## Chain 2: Iteration: 1500 / 5000 [ 30%]  (Warmup)
## Chain 2: Iteration: 2000 / 5000 [ 40%]  (Warmup)
## Chain 2: Iteration: 2500 / 5000 [ 50%]  (Warmup)
## Chain 2: Iteration: 2501 / 5000 [ 50%]  (Sampling)
## Chain 2: Iteration: 3000 / 5000 [ 60%]  (Sampling)
## Chain 2: Iteration: 3500 / 5000 [ 70%]  (Sampling)
## Chain 2: Iteration: 4000 / 5000 [ 80%]  (Sampling)
## Chain 2: Iteration: 4500 / 5000 [ 90%]  (Sampling)
## Chain 2: Iteration: 5000 / 5000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.013011 seconds (Warm-up)
## Chain 2:                0.013787 seconds (Sampling)
## Chain 2:                0.026798 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;bfd58553d9dcc0c7cf662698c8aa5414&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 2e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 5000 [  0%]  (Warmup)
## Chain 3: Iteration:  500 / 5000 [ 10%]  (Warmup)
## Chain 3: Iteration: 1000 / 5000 [ 20%]  (Warmup)
## Chain 3: Iteration: 1500 / 5000 [ 30%]  (Warmup)
## Chain 3: Iteration: 2000 / 5000 [ 40%]  (Warmup)
## Chain 3: Iteration: 2500 / 5000 [ 50%]  (Warmup)
## Chain 3: Iteration: 2501 / 5000 [ 50%]  (Sampling)
## Chain 3: Iteration: 3000 / 5000 [ 60%]  (Sampling)
## Chain 3: Iteration: 3500 / 5000 [ 70%]  (Sampling)
## Chain 3: Iteration: 4000 / 5000 [ 80%]  (Sampling)
## Chain 3: Iteration: 4500 / 5000 [ 90%]  (Sampling)
## Chain 3: Iteration: 5000 / 5000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.01286 seconds (Warm-up)
## Chain 3:                0.014255 seconds (Sampling)
## Chain 3:                0.027115 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;bfd58553d9dcc0c7cf662698c8aa5414&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 3e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 5000 [  0%]  (Warmup)
## Chain 4: Iteration:  500 / 5000 [ 10%]  (Warmup)
## Chain 4: Iteration: 1000 / 5000 [ 20%]  (Warmup)
## Chain 4: Iteration: 1500 / 5000 [ 30%]  (Warmup)
## Chain 4: Iteration: 2000 / 5000 [ 40%]  (Warmup)
## Chain 4: Iteration: 2500 / 5000 [ 50%]  (Warmup)
## Chain 4: Iteration: 2501 / 5000 [ 50%]  (Sampling)
## Chain 4: Iteration: 3000 / 5000 [ 60%]  (Sampling)
## Chain 4: Iteration: 3500 / 5000 [ 70%]  (Sampling)
## Chain 4: Iteration: 4000 / 5000 [ 80%]  (Sampling)
## Chain 4: Iteration: 4500 / 5000 [ 90%]  (Sampling)
## Chain 4: Iteration: 5000 / 5000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.012823 seconds (Warm-up)
## Chain 4:                0.015065 seconds (Sampling)
## Chain 4:                0.027888 seconds (Total)
## Chain 4:</code></pre>
<pre class="r"><code>print(fit, probs=c(0.1, 0.9))</code></pre>
<pre><code>## Inference for Stan model: bfd58553d9dcc0c7cf662698c8aa5414.
## 4 chains, each with iter=5000; warmup=2500; thin=1; 
## post-warmup draws per chain=2500, total post-warmup draws=10000.
## 
##         mean se_mean   sd    10%    90% n_eff Rhat
## theta   0.41    0.00 0.10   0.28   0.55  3823    1
## lp__  -15.40    0.01 0.73 -16.24 -14.89  3833    1
## 
## Samples were drawn using NUTS(diag_e) at Mon Jan 14 10:54:23 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<ul>
<li>We obtain</li>
</ul>
<pre><code>Inference for Stan model: a6e9032b5e2c0ad2011961902392006a.
4 chains, each with iter=5000; warmup=2500; thin=1; 
post-warmup draws per chain=2500, total post-warmup draws=10000.

        mean se_mean   sd    10%    90% n_eff Rhat
theta   0.23    0.00 0.09   0.12   0.34  3695    1
lp__  -12.30    0.01 0.71 -13.18 -11.80  3954    1

Samples were drawn using NUTS(diag_e)
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<pre class="r"><code># Extracting the posterior draws
theta_draws = extract(fit)$theta

# Calculating posterior mean (estimator)
 mean(theta_draws)</code></pre>
<pre><code>## [1] 0.4108909</code></pre>
<pre class="r"><code># Calculating posterior intervals
 quantile(theta_draws, probs=c(0.10, 0.90))</code></pre>
<pre><code>##       10%       90% 
## 0.2794651 0.5469013</code></pre>
<hr />
<pre class="r"><code>theta_draws_df = data.frame(list(theta = theta_draws))

plotpostre = ggplot(theta_draws_df, aes(x = theta)) +
geom_histogram(bins=20, color = &quot;gray&quot;)
plotpostre</code></pre>
<p><img src="/post/2019-01-22_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<hr />
<ul>
<li>RStan: MAP, penalized <em>MLE</em></li>
<li>Stan’s optimization for estimation; two views:</li>
<li>max posterior mode, also known as max a posteriori (<em>MAP</em>)</li>
<li>max penalized likelihood (<em>MLE</em>)</li>
</ul>
<pre class="r"><code>library(rstan)
N = 5
y = c(0,1,1,0,0)

model = stan_model(model_code=bern.stan)
mle = optimizing(model, data=c(&quot;N&quot;, &quot;y&quot;))</code></pre>
<pre class="r"><code>print(mle, digits=2)</code></pre>
<pre><code>## $par
## theta 
##   0.4 
## 
## $value
## [1] -3.4
## 
## $return_code
## [1] 0</code></pre>
<hr />
</div>
<div id="lotka-volterras-1927-model" class="section level4">
<h4>Lotka-Volterra’s (1927) Model</h4>
<ul>
<li><p>See
<a href="https://mc-stan.org/users/documentation/case-studies/lotka-volterra-predator-prey.html#data-lynx-and-hare-pelts-in-canada">Stan and Lotka-Volterra models</a></p></li>
<li><strong>population</strong>:
<ul>
<li><span class="math inline">\(u(t)\)</span> prey,</li>
<li><span class="math inline">\(v(t)\)</span> predator</li>
</ul></li>
</ul>
<p><span class="math display">\[\frac{d}{dt}u = (\alpha -\beta v)u\]</span></p>
<p><span class="math display">\[\frac{d}{dt}v=(-\gamma +\delta u)v\]</span></p>
<ul>
<li><span class="math inline">\(\alpha\)</span>: prey growth, intrinsic</li>
<li><span class="math inline">\(\beta\)</span>: prey shrinkage due to predation</li>
<li><span class="math inline">\(\gamma\)</span>: predator shrinkage, intrinsic</li>
<li><span class="math inline">\(\delta\)</span>: predator growth from predation</li>
</ul>
<hr />
</div>
<div id="lotka-volterra-in-stan" class="section level4">
<h4>Lotka-Volterra in Stan</h4>
<pre><code>real[] dz_dt(data real t,       // time 
  real[] z,                     // system state
  real[] theta,                 // parameters
  data real[] x_r,              // real data
  data int[] x_i) {             // integer data 
  
real u = z[1];                  // extract state
real v = z[2];
real alpha = theta[1];
real beta = theta[2];
real gamma = theta[3];
real delta = theta[4];

real du_dt = (alpha - beta * v) * u;
real dv_dt = (-gamma + delta * u) * v;
return { du_dt, dv_dt };
}</code></pre>
<ul>
<li>Known variables are observed
<ul>
<li><span class="math inline">\(y_{n,k}\)</span>: pelts for species <span class="math inline">\(k\)</span> at times <span class="math inline">\(t_{n}\)</span> for <span class="math inline">\(n \in 0:N\)</span></li>
<li>Unknown variables must be inferred (inverse problem)</li>
</ul></li>
<li>initial state: <span class="math inline">\(z_{k}^{init}\)</span>: initial population for <span class="math inline">\(k\)</span></li>
<li>subsequent states <span class="math inline">\(z_{n,k}\)</span>: population <span class="math inline">\(k\)</span> at time <span class="math inline">\(t_{n}\)</span></li>
<li>parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(\delta &gt; 0\)</span></li>
<li>Likelihood assumes errors are proportional (not additive)</li>
</ul>
<p><span class="math display">\[y_{n,k}\sim LogNormal(\hat{z}_{n,k}, \sigma_{k}),\]</span></p>
<ul>
<li>Equivalently:
<span class="math display">\[\log y_{n,k} = \log \widehat{z}_{n,k} + \epsilon_{n,k}\]</span></li>
</ul>
<p>with</p>
<p><span class="math display">\[\epsilon_{n,k} \sim Normal(0, \sigma_{k})\]</span></p>
<hr />
</div>
<div id="lotka-volterra-in-stan-data-parameters" class="section level4">
<h4>Lotka-Volterra in Stan (data, parameters)</h4>
<ul>
<li>Variables for known constants, observed data</li>
</ul>
<pre><code>data {
int&lt;lower = 0&gt; N;       // num measurements
real ts[N];             // measurement times &gt; 0
real y0[2];             // initial pelts
real&lt;lower=0&gt; y[N,2];   // subsequent pelts
}</code></pre>
<ul>
<li>Variables for unknown parameters</li>
</ul>
<pre><code>parameters {
real&lt;lower=0&gt; theta[4];    // alpha, beta, gamma, delta
real&lt;lower=0&gt; z0[2];       // initial population
real&lt;lower=0&gt; sigma[2];    // scale of prediction error
}</code></pre>
<hr />
</div>
<div id="lotka-volterra-in-stan-priors-likelihood" class="section level4">
<h4>Lotka-Volterra in Stan (priors, likelihood)</h4>
<ul>
<li>Sampling statements for priors and likelihood</li>
</ul>
<pre><code>model {
// priors
sigma ~ lognormal(0, 0.5);
theta[{1, 3}] ~ normal(1, 0.5);
theta[{2, 4}] ~ normal(0.05, 0.05);
z0[1] ~ lognormal(log(30), 5);
z0[2] ~ lognormal(log(5), 5);

// likelihood (lognormal)
for (k in 1:2) {
y0[k] ~ lognormal(log(z0[k]), sigma[k]);
y[ , k] ~ lognormal(log(z[, k]), sigma[k]);
}
}</code></pre>
<hr />
</div>
<div id="lotka-volterra-in-stan-solution-to-ode" class="section level4">
<h4>Lotka-Volterra in Stan (solution to ODE)</h4>
<ul>
<li>Define variables for populations predicted by ode, given
<ul>
<li>system function (dz_dt), initial populations (z0)</li>
<li>initial time (0.0), solution times (ts)</li>
<li>parameters (theta), data arrays</li>
<li>tolerances (1e-6, 1-e4), max iterations (1e3)</li>
</ul></li>
</ul>
<pre><code>transformed parameters {
real z[N, 2]
= integrate_ode_rk45(dz_dt, z0, 0.0, ts, theta,
rep_array(0.0, 0), rep_array(0, 0),
1e-6, 1e-4, 1e3);
}</code></pre>
<hr />
</div>
<div id="lotka-volterra-parameter-estimates" class="section level4">
<h4>Lotka-Volterra Parameter Estimates</h4>
<pre class="r"><code>fit = stan(model_code=lotka-volterra.stan, data = lynx_hare_data)

print(fit, c(&quot;theta&quot;, &quot;sigma&quot;), probs=c(0.1, 0.5, 0.9))</code></pre>
<hr />
<pre><code>mean     se_mean   sd  10%  50%  90% n_eff  Rhat
theta[1]    0.55    0 0.07 0.46 0.54 0.64 1168 1
theta[2]    0.03    0 0.00 0.02 0.03 0.03 1305 1
theta[3]    0.80    0 0.10 0.68 0.80 0.94 1117 1
theta[4]    0.02    0 0.00 0.02 0.02 0.03 1230 1
sigma[1]    0.29    0 0.05 0.23 0.28 0.36 2673 1
sigma[2]    0.29    0 0.06 0.23 0.29 0.37 2821 1</code></pre>
<hr />
</div>
<div id="obtained-results" class="section level4">
<h4>Obtained Results</h4>
<ul>
<li>Rhat near 1 signals convergence; n_eff is effective sample size</li>
<li>10%, … posterior quantiles; e.g., <span class="math inline">\(P[\alpha \in (0.46,0.64)|y]=0.8\)</span></li>
<li>posterior mean is Bayesian point estimate: <span class="math inline">\(\alpha = 0.55-\)</span></li>
<li>standard error in posterior mean estimate is 0 (with rounding)</li>
<li>posterior standard deviation of <span class="math inline">\(\alpha\)</span> estimated as 0.07</li>
</ul>
<hr />
<pre class="r"><code># FULL program

file = &quot;https://raw.githubusercontent.com/stan-dev/example-models/master/knitr/lotka-volterra/hudson-bay-lynx-hare.csv&quot;
# https://github.com/bblais/Systems-Modeling-Spring-2015-Notebooks/blob/master/data/Lynx%20and%20Hare%20Data/lynxhare.csv

lynx_hare_df = read.csv(file, header=TRUE, comment.char=&quot;#&quot;)
# lynx_hare_df

N = length(lynx_hare_df$Year) - 1
ts = 1:N
y_init = c(lynx_hare_df$Hare[1], lynx_hare_df$Lynx[1])
y = as.matrix(lynx_hare_df[2:(N + 1), 2:3])
y = cbind(y[ , 2], y[ , 1]); # hare, lynx order
lynx_hare_data = list(N, ts, y_init, y)

library(rstan)

thething = &quot;

functions {
real[] dz_dt(real t, real[] z, real[] theta,
             real[] x_r, int[] x_i) {
      real u = z[1];
      real v = z[2];
    
      real alpha = theta[1];
      real beta = theta[2];
      real gamma = theta[3];
      real delta = theta[4];
    
      real du_dt = (alpha - beta * v) * u;
      real dv_dt = (-gamma + delta * u) * v;
      return { du_dt, dv_dt };
    }
}

data {
  int&lt;lower = 0&gt; N;           // num measurements
  real ts[N];                 // measurement times &gt; 0
  real y_init[2];             // initial measured population
  real&lt;lower = 0&gt; y[N, 2];    // measured population at measurement times
}

parameters {
  real&lt;lower = 0&gt; theta[4];   // theta = {alpha, beta, gamma, delta}
  real&lt;lower = 0&gt; z_init[2];  // initial population
  real&lt;lower = 0&gt; sigma[2];   // error scale
}

transformed parameters {
  real z[N, 2]
    = integrate_ode_rk45(dz_dt, z_init, 0, ts, theta,
                         rep_array(0.0, 0), rep_array(0, 0),
                         1e-6, 1e-5, 1e3);
}

model {
  theta[{1, 3}] ~ normal(1, 0.5);
  theta[{2, 4}] ~ normal(0.05, 0.05);
  sigma ~ lognormal(-1, 1);
  z_init ~ lognormal(log(10), 1);
  for (k in 1:2) {
    y_init[k] ~ lognormal(log(z_init[k]), sigma[k]);
    y[ , k] ~ lognormal(log(z[, k]), sigma[k]);
  }
}
&quot;

fit = stan(model_code=thething, data=lynx_hare_data, iter=5000, chains=3, cores=3)

print(fit, pars=c(&quot;theta&quot;, &quot;sigma&quot;, &quot;z_init&quot;), 
      probs=c(0.1, 0.5, 0.9), digits = 3)</code></pre>
<pre><code>## Inference for Stan model: 31c444592f49190b5e10af31f66f8c9b.
## 3 chains, each with iter=5000; warmup=2500; thin=1; 
## post-warmup draws per chain=2500, total post-warmup draws=7500.
## 
##             mean se_mean    sd    10%    50%    90% n_eff Rhat
## theta[1]   0.549   0.002 0.065  0.469  0.547  0.631  1834    1
## theta[2]   0.028   0.000 0.004  0.023  0.028  0.033  2073    1
## theta[3]   0.797   0.002 0.093  0.688  0.790  0.917  1785    1
## theta[4]   0.024   0.000 0.004  0.020  0.024  0.029  1891    1
## sigma[1]   0.249   0.001 0.044  0.198  0.243  0.306  4810    1
## sigma[2]   0.251   0.001 0.044  0.200  0.245  0.308  4263    1
## z_init[1] 34.102   0.038 2.964 30.413 33.956 37.970  5994    1
## z_init[2]  5.935   0.009 0.526  5.289  5.905  6.605  3743    1
## 
## Samples were drawn using NUTS(diag_e) at Mon Jan 14 10:56:19 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<hr />
</div>
<div id="examples-of-stan-in-rstudio-cloud" class="section level4">
<h4>Examples of Stan in RStudio Cloud</h4>
<ul>
<li>Go to</li>
</ul>
<p><a href="https://andrewgelman.com/2018/10/12/stan-on-the-web-for-free-thanks-to-rstudio">Andrew Gelman’s bolg about Rstan in RStudio Cloud</a></p>
<p><a href="https://rstudio.cloud/project/56157">Examples Session in RStudio Cloud</a></p>
<ul>
<li>But I had problems to run these codes in the Cloud… :-(</li>
<li>Anyway, all examples can be donloaded from</li>
</ul>
<p><a href="https://github.com/stan-dev/example-models/archive/master.zip">Full examples of Andrew Gelman about Rstan</a></p>
</div>
