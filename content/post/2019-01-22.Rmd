---
title: "Stan: Probabilistic Modeling and Bayesian Inference"
authors: ["J. Miguel Mar√≠n"]
date: 2019-01-22
categories: ["R"]
tags: ["Stan", "Bayesian", "Functional programming"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
```

Stan is a probabilistic programming language for specifying statistical models. Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan can be called through R using the rstan package, and through Python using the pystan package. Both interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. In this talk it is shown a brief glance about the main properties of Stan. It is shown, also a couple of examples: first one related with a simple Bernoulli model and second one, about a Lotka-Volterra model based on ordinary differential equations.

***

#### Development Team of STAN

**Andrew Gelman** 
Bob Carpenter, Daniel Lee, Ben Goodrich,
Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Allen Riddell,
Marco Inacio, Jeffrey Arnold, Mitzi Morris, Rob Trangucci,
Rob Goedman, Brian Lau, Jonah Sol Gabry, Robert L. Grant,
Krzysztof Sakrejda, Aki Vehtari, Rayleigh Lei, Sebastian Weber,
Charles Margossian, Vincent Picaud, Imad Ali, Sean Talts,
Ben Bales, Ari Hartikainen, Matthijs Vakar, Andrew Johnson,
Dan Simpson

***

#### What is Stan?

- Stan is named in honor of *Stanislaw Ulam* (1909-1984): Co-inventor of the Monte Carlo method
- Stan is an imperative probabilistic programming language
- Stan program: defines a probability model
  + declares data and (constrained) parameter variables
  + defines log posterior (or penalized likelihood)
- Stan inference: fits model to data & makes predictions
  + MCMC for full Bayesian inference
  + VB for approximate Bayesian inference
  + MLE for penalized maximum likelihood estimation
  
***

#### WHAT STAN COMPUTES

- Draws from **posterior distributions**
- Stan performs Markov chain Monte Carlo sampling
- Produces sequence of draws $$\theta_{(1)} ,\theta_{(2)}, \ldots, \theta_{(M)}$$
- where each draw $\theta_{(i)}$ is marginally distributed according to the posterior $p(\theta|y)$
- Draws characterize posterior distributions
- Plot with histograms, kernel density estimates, etc.
- See a quick look about rstan
[Getting Started](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started)

***

#### INSTALL `rstan`

- Before installing `rstan` it is necessary to install Rtools
- See 
[Rtools stuff](https://cran.r-project.org/bin/windows/Rtools)

- Check whether the path (in Windows) is correctly fixed for all binaries of Rtools.
- If it is not the case, write in R:

```
install.packages(devtools)
library(devtools)
Sys.setenv(PATH = paste("C:\\Rtools\\bin", Sys.getenv("PATH"), sep=";"))
Sys.setenv(PATH = paste("C:\\Rtools\\mingw_64\\bin", Sys.getenv("PATH"), sep=";"))
```
- Then install `rstan`

```
install.packages(rstan)
```
***

#### BASIC SYNTAX in STAN

A Stan model is defined by five **program blocks data**

- transformed data
- parameters (required)
- transformed parameters
- model (required)
- generated quantities
  
***

- The **data** block reads external information

```
data {
  int N;
  int x[N];
  int offset;
}
```

***

- The **transformed data** block allows for preprocessing of the data 

```
transformed data {
  int y[N];
  for (n in 1:N)
    y[n] = x[n] - offset;
}
```

***

- The **parameters** block defines the sampling space

```
parameters {
real<lower=0> lambda1;
real<lower=0> lambda2;
}
```

***

- The **transformed parameters** block allows for parameter processing before the posterior is computed

```
transformed parameters {
real<lower=0> lambda;
lambda = lambda1 + lambda2;
}
```

***

- In the **model** block we define our posterior distributions

```
model {
y ~ poisson(lambda);
lambda1 ~ cauchy(0, 2.5);
lambda2 ~ cauchy(0, 2.5);
}
```

***

- Lastly, the **generated quantities** block allows for postprocessing

```
generated quantities {
int x_predict;
x_predict = poisson_rng(lambda) + offset;
}
```

***

- Stan has two primitive **types** and both can be bounded.
  + **int** is an integer type
  + **real** is a floating point precision type

```
int<lower=1> N;

real<upper=5> alpha;
real<lower=-1,upper=1> beta;

real gamma;
real<upper=gamma> zeta;
```

***

- Reals extend to linear algebra types

```
vector[10] a;     // Column vector
matrix[10, 1] b;

row_vector[10] c; // Row vector
matrix[1, 10] d;
```

- Arrays of int, reals, vectors, and matrices are available

```
real a[10];

vector[10] b;

matrix[10, 10] c;
```

- Stan also implements a variety of constrained types

```
simplex[5] theta;        // sum(theta) = 1

ordered[5] o;            // o[1] < ... < o[5]
positive_ordered[5] p;

corr_matrix[5] C;        // Symmetric and
cov_matrix[5] Sigma;     // positive-definite
```

***

- All the typical control and loop statements are available, too

```
if/then/else

for (i in 1:I)

while (i < I)
```

- There are two ways to modify the posterior

```
y ~ normal(0, 1);

increment_log_posterior(log_normal(y, 0, 1));
```

***

- Many sampling statements are *vectorized*

```
parameters {
real mu[N];
real<lower=0> sigma[N];
}


model {
 // for (n in 1:N)
 // y[n] ~ normal(mu[n], sigma[n]); 

y ~ normal(mu, sigma);  // Vectorized version
}
```

***

#### BAYESIAN APPROACH

Probability is **Epistemic**

- *John Stuart Mill* (Logic 1882, Part III, Ch. 2):
  + ... the probability of an event is not a quality of the event itself, but a mere name for the degree of ground which we, or some one else, have for expecting it.
- Every event is in itself certain, not probable; if we knew all, we should either know positively that it will happen, or positively that it will not.
- ... its probability to us means the degree of expectation of its occurrence, which we are warranted in entertaining by our present evidence.
  + Probabilities quantify uncertainty
  + Statistical reasoning is *counterfactual*

***

#### Repeated Binary Trial Model

- **Data**
  + $N\in \{0,1,\ldots \}$
  + $y_{n}\in \{0,1\}$ trial *n* success (known, modeled data)
- **Parameter**
  + $\theta \in \lbrack 0,1]$: chance of success (unknown)
- **Prior**
  + $p(\theta) = Uniform(\theta|0,1) = 1$

***

#### Repeated Binary Trial Model

- **Likelihood**

$$p(y|\theta )=\prod_{n=1}^{N}Bernoulli(y_{n}|\theta
)=Nn=\prod_{n=1}^{N}\theta ^{y_{n}}(1-\theta )^{1-y_{n}}$$

***

- **Posterior**

$$p(\theta |y)\propto p(\theta )p(y|\theta )$$

***

#### Stan Program


```{r}
bern.stan = 
"
data {
int<lower=0> N;                // number of trials
int<lower=0, upper=1> y[N];    // success on trial n
}

parameters {
real<lower=0, upper=1> theta;  // chance of success
}

model {
  theta ~ uniform(0, 1);       // prior
  y ~ bernoulli(theta);        // likelihood
}
"
```

***

#### R: Simulate some data


```{r}
# Generate data
 theta = 0.30
 N = 20
 y = rbinom(N, 1, 0.3)
 y
```

***

- Calculate MLE as sample mean from data

```{r}
 sum(y) / N
```

***

#### RStan: Bayesian Posterior

```{r}
library(rstan)

fit = stan(model_code=bern.stan, data = list(y = y, N = N), iter=5000)

print(fit, probs=c(0.1, 0.9))
```

- We obtain

``` 
Inference for Stan model: a6e9032b5e2c0ad2011961902392006a.
4 chains, each with iter=5000; warmup=2500; thin=1; 
post-warmup draws per chain=2500, total post-warmup draws=10000.

        mean se_mean   sd    10%    90% n_eff Rhat
theta   0.23    0.00 0.09   0.12   0.34  3695    1
lp__  -12.30    0.01 0.71 -13.18 -11.80  3954    1

Samples were drawn using NUTS(diag_e)
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
```

```{r}
# Extracting the posterior draws
theta_draws = extract(fit)$theta

# Calculating posterior mean (estimator)
 mean(theta_draws)

# Calculating posterior intervals
 quantile(theta_draws, probs=c(0.10, 0.90))
```

***

```{r}
theta_draws_df = data.frame(list(theta = theta_draws))

plotpostre = ggplot(theta_draws_df, aes(x = theta)) +
geom_histogram(bins=20, color = "gray")
plotpostre
```

***

- RStan: MAP, penalized *MLE*
- Stan's optimization for estimation; two views:
- max posterior mode, also known as max a posteriori (*MAP*)
- max penalized likelihood (*MLE*)


```{r}
library(rstan)
N = 5
y = c(0,1,1,0,0)

model = stan_model(model_code=bern.stan)
mle = optimizing(model, data=c("N", "y"))
```

```{r}
print(mle, digits=2)
```

***

#### Lotka-Volterra's (1927) Model

- See 
[Stan and Lotka-Volterra models](https://mc-stan.org/users/documentation/case-studies/lotka-volterra-predator-prey.html#data-lynx-and-hare-pelts-in-canada)

- **population**: 
  + $u(t)$ prey, 
  + $v(t)$ predator

$$\frac{d}{dt}u = (\alpha -\beta v)u$$

$$\frac{d}{dt}v=(-\gamma +\delta u)v$$

- $\alpha$: prey growth, intrinsic
- $\beta$: prey shrinkage due to predation
- $\gamma$: predator shrinkage, intrinsic
- $\delta$: predator growth from predation

***

#### Lotka-Volterra in Stan

```
real[] dz_dt(data real t,       // time 
  real[] z,                     // system state
  real[] theta,                 // parameters
  data real[] x_r,              // real data
  data int[] x_i) {             // integer data 
  
real u = z[1];                  // extract state
real v = z[2];
real alpha = theta[1];
real beta = theta[2];
real gamma = theta[3];
real delta = theta[4];

real du_dt = (alpha - beta * v) * u;
real dv_dt = (-gamma + delta * u) * v;
return { du_dt, dv_dt };
}
```

- Known variables are observed
  + $y_{n,k}$: pelts for species $k$ at times $t_{n}$ for $n \in 0:N$
  + Unknown variables must be inferred (inverse problem)
- initial state: $z_{k}^{init}$: initial population for $k$
- subsequent states $z_{n,k}$: population $k$ at time $t_{n}$
- parameters $\alpha$, $\beta$, $\gamma$, $\delta > 0$
- Likelihood assumes errors are proportional (not additive)

$$y_{n,k}\sim LogNormal(\hat{z}_{n,k}, \sigma_{k}),$$

- Equivalently: 
$$\log y_{n,k} = \log \widehat{z}_{n,k} + \epsilon_{n,k}$$ 

with 

$$\epsilon_{n,k} \sim Normal(0, \sigma_{k})$$

***

#### Lotka-Volterra in Stan (data, parameters)

- Variables for known constants, observed data

```
data {
int<lower = 0> N;       // num measurements
real ts[N];             // measurement times > 0
real y0[2];             // initial pelts
real<lower=0> y[N,2];   // subsequent pelts
}
```

- Variables for unknown parameters

```
parameters {
real<lower=0> theta[4];    // alpha, beta, gamma, delta
real<lower=0> z0[2];       // initial population
real<lower=0> sigma[2];    // scale of prediction error
}
```

***

#### Lotka-Volterra in Stan (priors, likelihood)

- Sampling statements for priors and likelihood

```
model {
// priors
sigma ~ lognormal(0, 0.5);
theta[{1, 3}] ~ normal(1, 0.5);
theta[{2, 4}] ~ normal(0.05, 0.05);
z0[1] ~ lognormal(log(30), 5);
z0[2] ~ lognormal(log(5), 5);

// likelihood (lognormal)
for (k in 1:2) {
y0[k] ~ lognormal(log(z0[k]), sigma[k]);
y[ , k] ~ lognormal(log(z[, k]), sigma[k]);
}
}
```

***

#### Lotka-Volterra in Stan (solution to ODE)

- Define variables for populations predicted by ode, given
  + system function (dz_dt), initial populations (z0)
  + initial time (0.0), solution times (ts)
  + parameters (theta), data arrays 
  + tolerances (1e-6, 1-e4), max iterations (1e3)

```
transformed parameters {
real z[N, 2]
= integrate_ode_rk45(dz_dt, z0, 0.0, ts, theta,
rep_array(0.0, 0), rep_array(0, 0),
1e-6, 1e-4, 1e3);
}
```

***

#### Lotka-Volterra Parameter Estimates


```{r, echo=TRUE, eval=FALSE}
fit = stan(model_code=lotka-volterra.stan, data = lynx_hare_data)

print(fit, c("theta", "sigma"), probs=c(0.1, 0.5, 0.9))
```

***

```
mean     se_mean   sd  10%  50%  90% n_eff  Rhat
theta[1]    0.55    0 0.07 0.46 0.54 0.64 1168 1
theta[2]    0.03    0 0.00 0.02 0.03 0.03 1305 1
theta[3]    0.80    0 0.10 0.68 0.80 0.94 1117 1
theta[4]    0.02    0 0.00 0.02 0.02 0.03 1230 1
sigma[1]    0.29    0 0.05 0.23 0.28 0.36 2673 1
sigma[2]    0.29    0 0.06 0.23 0.29 0.37 2821 1
```

***

#### Obtained Results



- Rhat near 1 signals convergence; n\_eff is effective sample size
- 10%, ... posterior quantiles; e.g., $P[\alpha \in (0.46,0.64)|y]=0.8$
- posterior mean is Bayesian point estimate: $\alpha = 0.55-$
- standard error in posterior mean estimate is 0 (with rounding)
- posterior standard deviation of $\alpha$ estimated as 0.07

***

```{r}
# FULL program

file = "https://raw.githubusercontent.com/stan-dev/example-models/master/knitr/lotka-volterra/hudson-bay-lynx-hare.csv"
# https://github.com/bblais/Systems-Modeling-Spring-2015-Notebooks/blob/master/data/Lynx%20and%20Hare%20Data/lynxhare.csv

lynx_hare_df = read.csv(file, header=TRUE, comment.char="#")
# lynx_hare_df

N = length(lynx_hare_df$Year) - 1
ts = 1:N
y_init = c(lynx_hare_df$Hare[1], lynx_hare_df$Lynx[1])
y = as.matrix(lynx_hare_df[2:(N + 1), 2:3])
y = cbind(y[ , 2], y[ , 1]); # hare, lynx order
lynx_hare_data = list(N, ts, y_init, y)

library(rstan)

thething = "

functions {
real[] dz_dt(real t, real[] z, real[] theta,
             real[] x_r, int[] x_i) {
	  real u = z[1];
	  real v = z[2];
	
	  real alpha = theta[1];
	  real beta = theta[2];
	  real gamma = theta[3];
	  real delta = theta[4];
	
	  real du_dt = (alpha - beta * v) * u;
	  real dv_dt = (-gamma + delta * u) * v;
	  return { du_dt, dv_dt };
	}
}

data {
  int<lower = 0> N;           // num measurements
  real ts[N];                 // measurement times > 0
  real y_init[2];             // initial measured population
  real<lower = 0> y[N, 2];    // measured population at measurement times
}

parameters {
  real<lower = 0> theta[4];   // theta = {alpha, beta, gamma, delta}
  real<lower = 0> z_init[2];  // initial population
  real<lower = 0> sigma[2];   // error scale
}

transformed parameters {
  real z[N, 2]
    = integrate_ode_rk45(dz_dt, z_init, 0, ts, theta,
                         rep_array(0.0, 0), rep_array(0, 0),
                         1e-6, 1e-5, 1e3);
}

model {
  theta[{1, 3}] ~ normal(1, 0.5);
  theta[{2, 4}] ~ normal(0.05, 0.05);
  sigma ~ lognormal(-1, 1);
  z_init ~ lognormal(log(10), 1);
  for (k in 1:2) {
    y_init[k] ~ lognormal(log(z_init[k]), sigma[k]);
    y[ , k] ~ lognormal(log(z[, k]), sigma[k]);
  }
}
"

fit = stan(model_code=thething, data=lynx_hare_data, iter=5000, chains=3, cores=3)

print(fit, pars=c("theta", "sigma", "z_init"), 
      probs=c(0.1, 0.5, 0.9), digits = 3)
```      

***

#### Examples of Stan in RStudio Cloud

- Go to

[Andrew Gelman's bolg about Rstan in RStudio Cloud](https://andrewgelman.com/2018/10/12/stan-on-the-web-for-free-thanks-to-rstudio)

[Examples Session in RStudio Cloud](https://rstudio.cloud/project/56157)

- But I had problems to run these codes in the Cloud... :-(
- Anyway, all examples can be donloaded from

[Full examples of Andrew Gelman about Rstan](https://github.com/stan-dev/example-models/archive/master.zip)
